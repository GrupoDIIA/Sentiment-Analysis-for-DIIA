{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1><center><font color=blue>D</font>escubrimiento de <font color=blue>I</font>nteracciones que <font color=blue>I</font>mpactan el <font color=blue>A</font>prendizaje (<font color=blue>D</font><font color=blue>I</font><font color=blue>I</font><font color=blue>A</font>)</center></h1>\n",
    "## <h2><center>Development of a software service for semantic patterns discovery impacting learning based on student's interaction in social networks</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This component allows to perform semantic analysis on text documents generated through the use of formal and informal learning platforms such as posts, comments and messages. It predicts the negative, positive or neutral polarity of the documents based on their lexical-syntactical structure.\n",
    "\n",
    "Example features:\n",
    "\n",
    "- All the programs (files *.py) were implemented in Python 2.7\n",
    "\n",
    "- The dataset used are part of a Spanish collection of tweets from TASS 2018 http://www.sepln.org/workshops/tass/\n",
    "\n",
    "- The Python packages required to run the programs are the following:\n",
    "  \n",
    "     - Numpy (classification) http://www.numpy.org/\n",
    "     - scikit-learn (classification) http://scikit-learn.org/stable/\n",
    "     - NLTK (NLP techniques) https://www.nltk.org/\n",
    "     - CLiPS pattern (NLP techniques) https://www.clips.uantwerpen.be/pattern\n",
    "\n",
    "The different steps used to clean, pre-process, represent and classify the set of text documents are described as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the development of this component the InterTASS Corpus (2017) was used. This courpus was created by the SEPLN (Spanish Society for Natural Language Processing) for the TASS-2017: Workshop on Semantic Analysis at SEPLN. \n",
    "It consists of tweets written in Spanish with sentiment annotated in a scale of 4 levels of polarity: positive (P), neutral (NEU), negative (N) and no sentiment (NONE). The corpus has three datasets:\n",
    "\n",
    "- Training: 1008 tweets.\n",
    "- Development: 506 tweets.\n",
    "- Test: 1899 tweets.\n",
    "\n",
    "The three datasets of the corpus are three XML files. An example may be found below: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tweet>\n",
    "    #The integer representation of the unique identifier for this Tweet.\n",
    "\t<tweetid>768228580673347584</tweetid>\n",
    "    #The user who posted the Tweet.\n",
    "\t<user>pacosan1269</user>\n",
    "    #The UTF-8 text of the Tweet.\n",
    "\t<content>Oye @luciaskaa te pones a hablar con @josecs02 como en los viejos tiempos y no avisas  @JuanAlfonso251 @CrisDazGar</content>\n",
    "    #The Tweet's timestamp.\n",
    "\t<date>2016-08-23 23:29:01</date>\n",
    "    #The language in which the Tweet's content is written. All corpus is written in Spanish (\"es\").\n",
    "\t<lang>es</lang>\n",
    "\t<sentiment>\n",
    "        #The polarity value attributed to the Tweet's content.\n",
    "\t\t<polarity><value>N</value></polarity>\n",
    "\t</sentiment>\n",
    "</tweet>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case that the original data document is in a different format than the desired, the first step is parsing.\n",
    "\n",
    "First, the the original training XML document is parsed into a plain TXT file using the Python ElementTree functionality. Considering a training sentiment file, the sentiment tags are obtained (positive, negative and\n",
    "neutral) except for the *NONE* tags, which were discarted. Bellow is described how to achieve this task:\n",
    "\n",
    "Note: the characters \"@-?@\" are used as delimiters in the input and output files. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The content (text) and polarity of the documents is obtained, except from the ones with a \"NONE\" polarity value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Module to generate and parse XML documents.\n",
    "import xml.etree.ElementTree as ET\n",
    "tweetDic=[]\n",
    "tree = ET.ElementTree(file=\"trainingOriginalFile.xml\")\n",
    "for x in tree.iter(tag='tweet'):\n",
    "   tempDict={}\n",
    "   for y in x:\n",
    "      tempDict[y.tag]=((y.text).replace('\\n',' ')).replace('\\n',' ')\n",
    "      if y.tag==\"sentiment\":\n",
    "        if (y[0][0].text) != \"NONE\":\n",
    "          tempDict[y.tag]=y[0][0].text\n",
    "          tweetDic.append(tempDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A TXT file that contains each document in one line is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Module to transcode text between different representations.\n",
    "import codecs\n",
    "with codecs.open(\"training.txt\",\"w\",\"utf-8\") as file:\n",
    "    for x in tweetDic:\n",
    "        file.write(x[\"tweetid\"]+\"@-?@\"+\n",
    "               x[\"user\"]+\"@-?@\"+\n",
    "               x[\"content\"]+\"@-?@\"+\n",
    "               x[\"date\"]+\"@-?@\"+\n",
    "               x[\"lang\"]+\"@-?@\"+\n",
    "x[\"sentiment\"]+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768213567418036224@-?@anahorxn@-?@@myendlesshazza a. que puto mal escribo  b. me sigo surrando help   3. ha quedado raro el \"cómetelo\" ahí JAJAJAJA@-?@2016-08-23 22:29:21@-?@es@-?@N\n",
      "\n",
      "768212591105703936@-?@martitarey13@-?@@estherct209 jajajaja la tuya y la d mucha gente seguro!! Pero yo no puedo sin mi melena me muero @-?@2016-08-23 22:25:29@-?@es@-?@N\n",
      "\n",
      "768221670255493120@-?@endlessmilerr@-?@Quiero mogollón a @AlbaBenito99 pero sobretodo por lo rápido que contesta a los wasaps @-?@2016-08-23 23:01:33@-?@es@-?@P\n",
      "\n",
      "768221021300264964@-?@JunoWTFL@-?@Vale he visto la tia bebiendose su regla y me hs dado muchs grima @-?@2016-08-23 22:58:58@-?@es@-?@N\n",
      "\n",
      "768220253730009091@-?@Alis_8496@-?@@Yulian_Poe @guillermoterry1 Ah. mucho más por supuesto! solo que lo incluyo. Me habías entendido mal @-?@2016-08-23 22:55:55@-?@es@-?@P\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Module to transcode text between different representations.\n",
    "import codecs\n",
    "#First 5 Tweets in the parsed training document.\n",
    "with codecs.open(\"training.txt\",\"r\",\"utf-8\") as file:\n",
    "    for x in range(5):\n",
    "        print file.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the test dataset, the same steps are followed. The original XML file is parsed into a plain TXT file using the Python ElementTree functionality; however, considering a test sentiment file, it should be noted that there is no extracted _\"sentiment\"_ tag. Therefore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Module to generate and parse XML documents.\n",
    "import xml.etree.ElementTree as ET\n",
    "#Module to transcode text between different representations.\n",
    "import codecs\n",
    "\n",
    "tweetDic=[]\n",
    "\n",
    "tree = ET.ElementTree(file=\"testOriginalFile.xml\")\n",
    "for x in tree.iter(tag='tweet'):\n",
    "   tempDict={}\n",
    "   for y in x:\n",
    "      tempDict[y.tag]=((y.text).replace('\\n',' ')).replace('\\n',' ')\n",
    "   tweetDic.append(tempDict)\n",
    "\n",
    "\n",
    "with codecs.open(\"test.txt\",\"w\",\"utf-8\") as file:\n",
    "    for x in tweetDic:\n",
    "        file.write(x[\"tweetid\"]+\"@-?@\"+\n",
    "               x[\"user\"]+\"@-?@\"+\n",
    "               x[\"content\"]+\"@-?@\"+\n",
    "               x[\"date\"]+\"@-?@\"+\n",
    "               x[\"lang\"]+\"\\n\")\n",
    "               #No \"sentiment\" tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "770567971701940224@-?@wikimiscojones@-?@@LonelySoad mientras que no te pillen la primera semana que es cuando tienes la nariz como un payaso @-?@2016-08-30 10:24:55@-?@es\n",
      "\n",
      "770503386789711872@-?@HLF_Metr4spt@-?@@ceemeese ya era hora de volver al csgo y dejares el padel bienvenida @-?@2016-08-30 06:08:17@-?@es\n",
      "\n",
      "770502863017635840@-?@AVazquez_C@-?@@mireiaescribano justo cuando se terminan las fiestas de verano, me viene genial  @-?@2016-08-30 06:06:12@-?@es\n",
      "\n",
      "770599972102348800@-?@minniecris@-?@@LuisMartinez22_ pensba q iba a hacer @wxplosive una reflexion profunda de las q me hace a mi pero @-?@2016-08-30 12:32:05@-?@es\n",
      "\n",
      "770599962216390656@-?@VI_Lelouch@-?@@Vic_Phantomhive Si lo encuentro, sin compañeros y barato, me iría hasta ahora mismo @-?@2016-08-30 12:32:02@-?@es\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Module to transcode text between different representations.\n",
    "import codecs\n",
    "#First 5 Tweets in the parsed test document.\n",
    "with codecs.open(\"test.txt\",\"r\",\"utf-8\") as file:\n",
    "    for x in range(5):\n",
    "        print file.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the documents are in the desired format, the training and test datasets are cleaned by removing all non-ASCII characters and URLs.\n",
    "\n",
    "- The diacritical marks are removed from the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Module to access to the Unicode Character Database which defines character properties for all Unicode characters.\n",
    "import unicodedata\n",
    "\n",
    "#Function that eliminates diacritical marks from Spanish words. \n",
    "def remove_accents(s):\n",
    "    try:\n",
    "        return ''.join((c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn'))\n",
    "    except TypeError:\n",
    "        return \"\"     \n",
    "    except UnicodeDecodeError:\n",
    "        return \"\"\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All URLs are removed from each document, lowercases and then collected into a new file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Module to transcode text between different representations.\n",
    "import codecs\n",
    "#Regular expressions module to search within and change text using formal patterns.\n",
    "import re\n",
    "#Pattern.es is a web mining module with natural language processing (NLP) tools for Spanish.\n",
    "#The parse() function annotates words in the given string with their part-of-speech (POS) tags.\n",
    "#The split() function takes the output of parse() and returns a Text.\n",
    "from pattern.es import parse, split\n",
    "try:\n",
    "    trainingList=[]\n",
    "    with codecs.open(\"training.txt\",\"r\",\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            elementList=line.split(\"@-?@\")\n",
    "            #The URLs are removed by matching and replacing a regular expression pattern with a space.\n",
    "            TextWithoutURL=re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''',\n",
    "                               '',elementList[2], flags=re.MULTILINE)\n",
    "            elements=TextWithoutURL.split(\" \")\n",
    "            wordList=[]\n",
    "            for x in elements:\n",
    "                #All non-ASCII characters are removed by matching and replacing a regular expression pattern with a space.\n",
    "                #Diacritical marks are removed through the remove_accents function.\n",
    "                #Letters are lowercased.\n",
    "                word= re.sub('[^A-Za-z0-9 ]+', '',remove_accents(x.lower()))\n",
    "                #After the previous formatting, words are stored in a list.\n",
    "                if len(word)>0:\n",
    "                    wordList.append(word)\n",
    "            #A TXT file containing each document in one line is generated.\n",
    "            trainingList.append(elementList[0]+\"@-?@\"+\n",
    "                                  elementList[1]+\"@-?@\"+\n",
    "                                  \" \".join(wordList)+\"@-?@\"+\n",
    "                                  elementList[3]+\"@-?@\"+\n",
    "                                  elementList[4]+\"@-?@\"+\n",
    "                                  elementList[5])        \n",
    "\n",
    "    with codecs.open(\"trainingClean.txt\",\"w\",\"utf-8\") as file:\n",
    "         for a in trainingList:\n",
    "                file.write(a)\n",
    "\n",
    "#Error handling.\n",
    "except IOError as (errno, strerror):\n",
    "    print \"Input/output error ({0}): {1}\".format(errno, strerror)\n",
    "\n",
    "except:\n",
    "    print \"Unexpected error:\", sys.exc_info()[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768213567418036224@-?@anahorxn@-?@myendlesshazza a que puto mal escribo b me sigo surrando help 3 ha quedado raro el cometelo ahi jajajaja@-?@2016-08-23 22:29:21@-?@es@-?@N\n",
      "\n",
      "768212591105703936@-?@martitarey13@-?@estherct209 jajajaja la tuya y la d mucha gente seguro pero yo no puedo sin mi melena me muero@-?@2016-08-23 22:25:29@-?@es@-?@N\n",
      "\n",
      "768221670255493120@-?@endlessmilerr@-?@quiero mogollon a albabenito99 pero sobretodo por lo rapido que contesta a los wasaps@-?@2016-08-23 23:01:33@-?@es@-?@P\n",
      "\n",
      "768221021300264964@-?@JunoWTFL@-?@vale he visto la tia bebiendose su regla y me hs dado muchs grima@-?@2016-08-23 22:58:58@-?@es@-?@N\n",
      "\n",
      "768220253730009091@-?@Alis_8496@-?@yulianpoe guillermoterry1 ah mucho mas por supuesto solo que lo incluyo me habias entendido mal@-?@2016-08-23 22:55:55@-?@es@-?@P\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Module to transcode text between different representations.\n",
    "import codecs\n",
    "#First 5 Tweets in the clean training document.\n",
    "with codecs.open(\"trainingClean.txt\",\"r\",\"utf-8\") as file:\n",
    "    for x in range(5):\n",
    "        print file.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test dataset is cleaned following the same steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Module to transcode text between different representations.\n",
    "import codecs\n",
    "#Regular expressions module to search within and change text using formal patterns.\n",
    "import re\n",
    "#Module to access to the Unicode Character Database which defines character properties for all Unicode characters.\n",
    "import unicodedata\n",
    "#Module that implements several types of pseudorandom number generators.\n",
    "import random\n",
    "\n",
    "#Function that eliminates diacritical marks from Spanish words. \n",
    "def remove_accents(s):\n",
    "    try:\n",
    "            return ''.join((c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn'))\n",
    "    except TypeError:\n",
    "            return \"\"     \n",
    "    except UnicodeDecodeError:\n",
    "            return \"\"\n",
    "    except:\n",
    "            return \"\"\n",
    "try:\n",
    "    testList=[]\n",
    "    with codecs.open(\"test.txt\",\"r\",\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            elementList=line.split(\"@-?@\")\n",
    "            #The URLs are removed by matching and replacing a regular expression pattern with a space.\n",
    "            TextWithoutURL=re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''',\n",
    "                               '',elementList[2], flags=re.MULTILINE)\n",
    "            elements=TextWithoutURL.split(\" \")\n",
    "            wordList=[]\n",
    "            for x in elements:\n",
    "                #All non-ASCII characters are removed by matching and replacing a regular expression pattern with a space.\n",
    "                #Diacritical marks are removed through the remove_accents function.\n",
    "                #Letters are lowercased.\n",
    "                word= re.sub('[^A-Za-z0-9 ]+', '',remove_accents(x.lower()))\n",
    "                #After the previous formatting, words are stored in a list.\n",
    "                if len(word)>0:\n",
    "                    wordList.append(word)\n",
    "            #A TXT file containing each document in one line is generated.\n",
    "            testList.append(elementList[0]+\"@-?@\"+\n",
    "                                  elementList[1]+\"@-?@\"+\n",
    "                                  \" \".join(wordList)+\"@-?@\"+\n",
    "                                  elementList[3]+\"@-?@\"+\n",
    "                                  elementList[4])\n",
    "                                  #No \"sentiment\" tag.\n",
    "\n",
    "    with codecs.open(\"testClean.txt\",\"w\",\"utf-8\") as file:\n",
    "         for a in testList:\n",
    "             file.write(a)\n",
    "\n",
    "#Error handling.             \n",
    "except IOError as (errno, strerror):\n",
    "    print \"Input/output error ({0}): {1}\".format(errno, strerror)\n",
    "\n",
    "except:\n",
    "    print \"Unexpected error:\", sys.exc_info()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "770567971701940224@-?@wikimiscojones@-?@lonelysoad mientras que no te pillen la primera semana que es cuando tienes la nariz como un payaso@-?@2016-08-30 10:24:55@-?@es\n",
      "\n",
      "770503386789711872@-?@HLF_Metr4spt@-?@ceemeese ya era hora de volver al csgo y dejares el padel bienvenida@-?@2016-08-30 06:08:17@-?@es\n",
      "\n",
      "770502863017635840@-?@AVazquez_C@-?@mireiaescribano justo cuando se terminan las fiestas de verano me viene genial@-?@2016-08-30 06:06:12@-?@es\n",
      "\n",
      "770599972102348800@-?@minniecris@-?@luismartinez22 pensba q iba a hacer wxplosive una reflexion profunda de las q me hace a mi pero@-?@2016-08-30 12:32:05@-?@es\n",
      "\n",
      "770599962216390656@-?@VI_Lelouch@-?@vicphantomhive si lo encuentro sin companeros y barato me iria hasta ahora mismo@-?@2016-08-30 12:32:02@-?@es\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Module to transcode text between different representations.\n",
    "import codecs\n",
    "#First 5 Tweets in the clean test document.\n",
    "with codecs.open(\"testClean.txt\",\"r\",\"utf-8\") as file:\n",
    "    for x in range(5):\n",
    "        print file.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocessing, the next step is to collect the textual features for the construction of a classification model. Here, the fequency of occurrence of N-grams is obtained. N-grams are obtained using the Ngram functionality of the Natural Language Toolkit (NLTK). For this component, trigrams are used. \n",
    "\n",
    "- The N-grams associated to each text document with their frequency are obtained and ordered from most to least frequent in a feature set file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Module to transcode text between different representations.\n",
    "import codecs\n",
    "#Module that provides system-specific configuration and operations.\n",
    "import sys\n",
    "#Regular expressions module to search within and change text using formal patterns.\n",
    "import re\n",
    "#Module that serves as a functional interface to built-in operators.\n",
    "import operator\n",
    "#The Natural Language Toolkit is a module with natural language processing (NLP) resources and corpora.\n",
    "import nltk\n",
    "#Word_tokenize is a tokenizer that divides a string into a list of substrings.\n",
    "from nltk import word_tokenize\n",
    "#Ngrams returns the ngrams generated from a sequence of items\n",
    "from nltk.util import ngrams\n",
    "\n",
    "#Determine the N-gram window size.\n",
    "NgramType=3\n",
    "\n",
    "try:\n",
    "    nGramFeatureSet={}\n",
    "    with codecs.open(\"trainingClean.txt\",\"r\",\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            elementList=line.split(\"@-?@\")\n",
    "            #Words are tokenized.\n",
    "            words = nltk.word_tokenize(elementList[2])\n",
    "            #N-grams are obtained.\n",
    "            nGrams=ngrams(words,NgramType)\n",
    "            #N-grams are collected and their frequency is calculated.\n",
    "            for nGram in nGrams:\n",
    "                nGram=' '.join(e for e in nGram)   \n",
    "                if nGram in nGramFeatureSet:\n",
    "                    nGramFeatureSet[nGram]=nGramFeatureSet[nGram]+1\n",
    "                else:\n",
    "                    nGramFeatureSet[nGram]=1     \n",
    "    #N-grams are sorted from most to least frequent.\n",
    "    nGramFeatureSetSort = sorted(nGramFeatureSet.items(),\n",
    "                                 key=operator.itemgetter(1),\n",
    "                                 reverse=True)\n",
    "\n",
    "#A TXT file containing each document in one line is generated.\n",
    "    with codecs.open(\"feature3Gram.txt\",\"w\",\"utf-8\") as file:\n",
    "         for a in nGramFeatureSetSort:\n",
    "             file.write(a[0]+\"@-?@\"+str(a[1])+\"\\n\")\n",
    "\n",
    "#Error handling.\n",
    "except IOError as (errno, strerror):\n",
    "    print \"Input/output error ({0}): {1}\".format(errno, strerror)\n",
    "\n",
    "except:\n",
    "    print \"Unexpected error:\", sys.exc_info()[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es que es@-?@9\n",
      "\n",
      "que no me@-?@7\n",
      "\n",
      "no pasa nada@-?@5\n",
      "\n",
      "y no me@-?@5\n",
      "\n",
      "me voy a@-?@5\n",
      "\n",
      "que tengas un@-?@4\n",
      "\n",
      "va a ser@-?@4\n",
      "\n",
      "no he podido@-?@4\n",
      "\n",
      "a mi me@-?@4\n",
      "\n",
      "es que no@-?@4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Module to transcode text between different representations.\n",
    "import codecs\n",
    "#First 10 training trigrams.\n",
    "with codecs.open(\"feature3Gram.txt\",\"r\",\"utf-8\") as file:\n",
    "    for x in range(10):\n",
    "        print file.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step to obtain the texts' polarity is to transform each text (training) into a vector representation taking the frequency of occurrence of the \"N\" best N-grams. For this component, the 150 most frequent trigrams are used.\n",
    "\n",
    "- The N most frequent N-grams are obtained. \n",
    "- Each text is transformed into a vector representation based on the frequency of occurrence of the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Module to transcode text between different representations.\n",
    "import codecs\n",
    "#Regular expressions module to search within and change text using formal patterns.\n",
    "import re\n",
    "#Module that provides system-specific configuration and operations.\n",
    "import sys\n",
    "#Module that serves as a functional interface to built-in operators.\n",
    "import operator\n",
    "#The Natural Language Toolkit is a module with natural language processing (NLP) resources and corpora.\n",
    "import nltk\n",
    "#Word_tokenize is a tokenizer that divides a string into a list of substrings.\n",
    "from nltk import word_tokenize\n",
    "#Ngrams returns the ngrams generated from a sequence of items\n",
    "from nltk.util import ngrams\n",
    "#The Collections module includes container data types beyond the built-in types list, dict, and tuple.\n",
    "#A Counter is a container that keeps track of how many times equivalent values are added.\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "#Determine the N-gram window size.\n",
    "NgramType=3\n",
    "#Determine the amount of features to select.\n",
    "NgramNumber=150\n",
    "\n",
    "try: \n",
    "    nGramFeatureSet=[]\n",
    "    counter=0\n",
    "    with codecs.open(\"feature3Gram.txt\",\"r\",\"utf-8\") as file:\n",
    "        #The \"N\" features are collected.\n",
    "        for line in file:\n",
    "            counter=counter+1\n",
    "            if counter <= NgramNumber:\n",
    "                elementsList=line.split(\"@-?@\")\n",
    "                nGramFeatureSet.append(elementsList[0])\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    vectorList=[]\n",
    "    sentimentTags=[]\n",
    "    with codecs.open(\"trainingClean.txt\",\"r\",\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            nGramList=[]\n",
    "            vector=[]\n",
    "            elementsList=line.split(\"@-?@\")\n",
    "            #Sentiment tags are collected.\n",
    "            sentimentTags.append(elementsList[5])\n",
    "            #The text is tokenized.\n",
    "            words = nltk.word_tokenize(elementsList[2])\n",
    "            #N-grams are obtained.\n",
    "            nGrams=ngrams(words,NgramType)\n",
    "            #N-grams are collected.\n",
    "            for nGram in nGrams:\n",
    "                nGram=' '.join(e for e in nGram)\n",
    "                nGramList.append(nGram)\n",
    "            #Texts are transformed into vectors.\n",
    "            for nGram in nGramFeatureSet:\n",
    "                vector.append(nGramList.count(nGram))    \n",
    "            vectorList.append(vector)    \n",
    "                   \n",
    "#A TXT file containing each document in one line is generated.\n",
    "    with codecs.open(\"trainingVectors-3-150.txt\",\"w\",\"utf-8\") as file:\n",
    "         for x,y in zip(vectorList,sentimentTags):\n",
    "             file.write(','.join(map(str, x))+\",\"+y)\n",
    "\n",
    "#Error handling.\n",
    "except IOError as (errno, strerror):\n",
    "    print \"Input/output error ({0}): {1}\".format(errno, strerror)\n",
    "\n",
    "except:\n",
    "    print \"Unexpected error:\", sys.exc_info()[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same actions are taken for the test documents to transform them into vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Module to transcode text between different representations.\n",
    "import codecs\n",
    "#Regular expressions module to search within and change text using formal patterns.\n",
    "import re\n",
    "#Module that provides system-specific configuration and operations.\n",
    "import sys\n",
    "#Module that serves as a functional interface to built-in operators.\n",
    "import operator\n",
    "#The Natural Language Toolkit is a module with natural language processing (NLP) resources and corpora.\n",
    "import nltk\n",
    "#Word_tokenize is a tokenizer that divides a string into a list of substrings.\n",
    "from nltk import word_tokenize\n",
    "#Ngrams returns the ngrams generated from a sequence of items\n",
    "from nltk.util import ngrams\n",
    "#The Collections module includes container data types beyond the built-in types list, dict, and tuple.\n",
    "#A Counter is a container that keeps track of how many times equivalent values are added.\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "#Determine the N-gram window size.\n",
    "NgramType=3\n",
    "#Determine the amount of features to select.\n",
    "NgramNumber=150\n",
    "\n",
    "\n",
    "try: \n",
    "    nGramFeatureSet=[]\n",
    "    counter=0\n",
    "    with codecs.open(\"feature3Gram.txt\",\"r\",\"utf-8\") as file:\n",
    "        #The \"N\" features are collected.\n",
    "        for line in file:\n",
    "            counter=counter+1\n",
    "            if counter <= NgramNumber:\n",
    "                elementsList=line.split(\"@-?@\")\n",
    "                nGramFeatureSet.append(elementsList[0])\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    vectorList=[]\n",
    "    with codecs.open(\"testClean.txt\",\"r\",\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            nGramList=[]\n",
    "            vector=[]\n",
    "            elementsList=line.split(\"@-?@\")\n",
    "            #The text is tokenized.\n",
    "            words = nltk.word_tokenize(elementsList[2])\n",
    "            #N-grams are obtained.\n",
    "            nGrams=ngrams(words,NgramType)\n",
    "            #N-grams are collected.\n",
    "            for nGram in nGrams:\n",
    "                nGram=' '.join(e for e in nGram)\n",
    "                nGramList.append(nGram)\n",
    "            #Texts are transformed into vectors.\n",
    "            for nGram in nGramFeatureSet:\n",
    "                vector.append(nGramList.count(nGram))    \n",
    "            vectorList.append(vector)    \n",
    "\n",
    "#A TXT file containing each document in one line is generated.\n",
    "    with codecs.open(\"testVectors-3-150.txt\",\"w\",\"utf-8\") as file:\n",
    "         for x in vectorList:\n",
    "             file.write(','.join(map(str, x))+\"\\n\")\n",
    "\n",
    "#Error handling.            \n",
    "except IOError as (errno, strerror):\n",
    "    print \"Input/output error ({0}): {1}\".format(errno, strerror)\n",
    "\n",
    "except:\n",
    "    print \"Unexpected error:\", sys.exc_info()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the vectors previously created are used to construct a predictive model of the sentiment (positive, negative or neutral) of a text based in the supervised learning theory.\n",
    "\n",
    "- A model is created from training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Module to transcode text between different representations.\n",
    "import codecs\n",
    "#Regular expressions module to search within and change text using formal patterns.\n",
    "import re\n",
    "#Module that provides system-specific configuration and operations.\n",
    "import sys\n",
    "#Module for Python object serialization.\n",
    "import pickle\n",
    "#NumPy is a package for scientific computing that provides some of the highly optimized data structures. \n",
    "import numpy as np\n",
    "#The scikit-learn module provides tools for data mining and data analysis.\n",
    "#Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n",
    "from sklearn import svm\n",
    "#The sklearn.metrics module implements several loss, score, and utility functions to measure classification performance. \n",
    "#Precision_score computes the precision classification score.\n",
    "from sklearn.metrics import precision_score\n",
    "#Accuracy_score computes the accuracy classification score.\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Determine the N-gram window size.\n",
    "NgramType=3\n",
    "#Determine the amount of features to select.\n",
    "NgramNumber=150\n",
    "\n",
    "\n",
    "#try:\n",
    "temp=[]\n",
    "temp2=[]\n",
    "trainingVectors=[]\n",
    "testVectors=[]\n",
    "sentimentTag=[]\n",
    "goldstandarTag=[]\n",
    "testResults=[]\n",
    "\n",
    "with codecs.open(\"trainingVectors-3-150.txt\",\"r\",\"utf-8\") as file:\n",
    "        for line in file:\n",
    "          elements=line.split(\",\")\n",
    "          vector=elements[0:len(elements)-1]\n",
    "          trainingVectors.append([int(i) for i in vector])\n",
    "          sentimentTag.append((elements[len(elements)-1]).rstrip())\n",
    "\n",
    "with codecs.open(\"testVectors-3-150.txt\",\"r\",\"utf-8\") as file:\n",
    "        for line in file:\n",
    "          vector=line.split(\",\")\n",
    "          vector[len(vector)-1]=vector[len(vector)-1].rstrip()\n",
    "          temp.append([[int(i) for i in vector]])\n",
    "\n",
    "#A TXT file containing each document in one line is generated.\n",
    "with codecs.open(\"goldStandar.qrel\",\"r\",\"utf-8\") as file:\n",
    "        for line in file:\n",
    "          elements=line.split(\"\t\")\n",
    "          elements[1]=elements[1].rstrip()\n",
    "          if elements[1]!=\"NONE\":\n",
    "              temp2.append(elements[1])\n",
    "\n",
    "#The NONE elements are removed from the dataset\n",
    "for x,y in zip(temp,temp2):\n",
    "    if y!=\"NONE\":\n",
    "        testVectors.append(x)\n",
    "        goldstandarTag.append(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each test vector a sentiment is predicted using the classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.472\n",
      "Model micro presicion: 0.472\n",
      "Model macro presicion: 0.157333333333\n",
      "Model weighted presicion: 0.222784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#Module to transcode text between different representations.\n",
    "import codecs\n",
    "#Regular expressions module to search within and change text using formal patterns.\n",
    "import re\n",
    "#Module that provides system-specific configuration and operations.\n",
    "import sys\n",
    "#Module for Python object serialization.\n",
    "import pickle\n",
    "#NumPy is a package for scientific computing that provides some of the highly optimized data structures. \n",
    "import numpy as np\n",
    "#The scikit-learn module provides tools for data mining and data analysis.\n",
    "#Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n",
    "from sklearn import svm\n",
    "#The sklearn.metrics module implements several loss, score, and utility functions to measure classification performance. \n",
    "#Precision_score computes the precision classification score.\n",
    "from sklearn.metrics import precision_score\n",
    "#Accuracy_score computes the accuracy classification score.\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# SVC with polynomial (degree 3) kernel\n",
    "C = 1.0\n",
    "clf= poly_svc = svm.SVC(kernel='poly', degree=3, C=C)\n",
    "#training phase (model construction)\n",
    "clf.fit(trainingVectors, sentimentTag)\n",
    "#Model serialization\n",
    "pickle.dump(clf, open(\"Model-3-150.txt\", \"wb\"))\n",
    "#in case of loading the object use: dump = pickle.load(\"Model-3-150.txt\")\n",
    "    \n",
    "#Testing phase\n",
    "for vector in testVectors:\n",
    "    testResults.append(clf.predict(vector)[0])\n",
    "    \n",
    "print \"Model Accuracy: \"+str(accuracy_score(goldstandarTag,testResults))\n",
    "print \"Model micro presicion: \"+str(precision_score(goldstandarTag,testResults, average='micro'))\n",
    "print \"Model macro presicion: \"+str(precision_score(goldstandarTag,testResults, average='macro'))\n",
    "print \"Model weighted presicion: \"+str(precision_score(goldstandarTag,testResults, average='weighted'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
